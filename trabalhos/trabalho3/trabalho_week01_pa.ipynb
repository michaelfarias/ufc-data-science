{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train.npy', 'rb') as fin:\n",
    "    X = np.load(fin)#features\n",
    "    \n",
    "with open('target.npy', 'rb') as fin:\n",
    "    y = np.load(fin)#labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:660, :]\n",
    "y_train = y[:660,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X[660:, :]\n",
    "y_test = y[660:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((660, 2), (660,), (166, 2), (166,), (826, 2), (826,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape,X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand(X):\n",
    "    \"\"\"\n",
    "    Adds quadratic features. \n",
    "    This expansion allows your linear model to make non-linear separation.\n",
    "    \n",
    "    For each sample (row in matrix), compute an expanded row:\n",
    "    [feature0, feature1, feature0^2, feature1^2, feature0*feature1, 1]\n",
    "    \n",
    "    :param X: matrix of features, shape [n_samples,2]\n",
    "    :returns: expanded features of shape [n_samples,6]\n",
    "    \"\"\"\n",
    "    X_expanded = np.zeros((X.shape[0], 6))\n",
    "    X_expanded[:,0] = X[ :, 0 ]\n",
    "    X_expanded[:,1] = X[ :, 1 ]\n",
    "    X_expanded[:,2] = X[ :, 0 ] **2\n",
    "    X_expanded[:,3] = X[ :, 1 ] **2\n",
    "    X_expanded[:,4] = X[ :, 0 ] * X[ :, 1 ]\n",
    "    X_expanded [:,5]= np.ones(X.shape[0])\n",
    "    \n",
    "    return X_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probability(X, w):\n",
    "    \"\"\"\n",
    "    Given input features and weights\n",
    "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
    "        \n",
    "    Don't forget to use expand(X) function (where necessary) in this and subsequent functions.\n",
    "    \n",
    "    :param X: feature matrix X of shape [n_samples,6] (expanded)\n",
    "    :param w: weight vector w of shape [6] for each of the expanded features\n",
    "    :returns: an array of predicted probabilities in [0,1] interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 1 / (1 + (np.exp(-np.dot(X,w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w):\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute scalar loss function L using formula above.\n",
    "    Keep in mind that our loss is averaged over all samples (rows) in X.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_prime = probability(X,w)\n",
    "    error = -(y* np.log(y_prime) + ( 1 - y) * np.log(1 - y_prime))\n",
    "    \n",
    "    \n",
    "    return np.mean(error) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_grad(X, y, w):\n",
    "    global y_train\n",
    "    \"\"\"\n",
    "    Given feature matrix X [n_samples,6], target vector [n_samples] of 1/0,\n",
    "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
    "    Keep in mind that our loss is averaged over all samples (rows) in X.\n",
    "    \"\"\"\n",
    "    y_prime = probability(X,w)\n",
    "    diff = np.expand_dims(y_prime - y, -1)\n",
    "    \n",
    "    return np.mean( X *diff , axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(X, y):\n",
    "\n",
    "    X_expanded = expand(X)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    w = np.array([0, 0, 0, 0, 0, 1])\n",
    "\n",
    "    eta= 0.1 # learning rate\n",
    "\n",
    "    n_iter = 100\n",
    "    batch_size = 4\n",
    "    loss = np.zeros(n_iter)\n",
    "   \n",
    "\n",
    "    for i in range(n_iter):\n",
    "         ind = np.random.choice(X_expanded.shape[0], batch_size)\n",
    "    #     loss[i] = compute_loss(X_expanded, y_train, w)\n",
    "\n",
    "        # Keep in mind that compute_grad already does averaging over batch for you!\n",
    "        # TODO:<your code here>\n",
    "\n",
    "         w = w - eta *compute_grad(X_expanded[ind, :], y[ind], w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hit_matrix = np.array([])\n",
    "hit_class_a = np.array([])\n",
    "hit_class_b = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculation of fees (i) average, (ii) minimum and (iii) maximum. And calculation of the standard deviation.\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    np.random.shuffle(X)\n",
    "    np.random.shuffle(y)\n",
    "    \n",
    "    X_train = X[:660, :]\n",
    "    y_train = y[:660,]\n",
    "\n",
    "    X_test = X[660:, :]\n",
    "    y_test = y[660:,]\n",
    "    \n",
    "    w = sgd(X_train, y_train)\n",
    "    X_test = expand(X_test)\n",
    "    y_test_prime = probability(X_test, w)\n",
    "    \n",
    "    y_test_prime[y_test_prime > 0.5 ]  = 1\n",
    "    y_test_prime[y_test_prime < 0.5] = 0\n",
    "    \n",
    "    average = y_test_prime == y_test\n",
    "    \n",
    "    hit_matrix = np.append(hit_matrix, np.count_nonzero(average))\n",
    "    \n",
    "    average_class_a = y_test_prime == 0\n",
    "    average_class_a = average_class_a == y_test\n",
    "   \n",
    "    hit_class_a = np.append(hit_class_a, np.count_nonzero(average_class_a))\n",
    "   \n",
    "    average_class_b = y_test_prime == 1\n",
    "    average_class_b = average_class_b == y_test\n",
    "   \n",
    "    hit_class_b = np.append(hit_class_b, np.count_nonzero(average_class_b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de acertos: \n",
      "-Média:  99.8069306931\n",
      "-Mínima 79.0\n",
      "-Máxima:  116.0\n",
      "-Desvio padrão:  6.08336080618\n"
     ]
    }
   ],
   "source": [
    "print (\"Media de acertos: \")\n",
    "print(\"-Média: \", np.mean(hit_matrix))\n",
    "print (\"-Mínima\", np.min(hit_matrix))\n",
    "print (\"-Máxima: \", np.max(hit_matrix))\n",
    "print (\"-Desvio padrão: \", np.std(hit_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de acertos da classe A(0): \n",
      "-Média:  66.2139303483\n",
      "-Mínima 50.0\n",
      "-Máxima:  87.0\n",
      "-Desvio padrão:  6.09126321361\n"
     ]
    }
   ],
   "source": [
    "print (\"Media de acertos da classe A(0): \")\n",
    "print(\"-Média: \", np.mean(hit_class_a))\n",
    "print (\"-Mínima\", np.min(hit_class_a))\n",
    "print (\"-Máxima: \", np.max(hit_class_a))\n",
    "print (\"-Desvio padrão: \", np.std(hit_class_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de acertos da classe B(1): \n",
      "-Média:  99.7860696517\n",
      "-Mínima 79.0\n",
      "-Máxima:  116.0\n",
      "-Desvio padrão:  6.09126321361\n"
     ]
    }
   ],
   "source": [
    "print (\"Media de acertos da classe B(1): \")\n",
    "print(\"-Média: \", np.mean(hit_class_b))\n",
    "print (\"-Mínima\", np.min(hit_class_b))\n",
    "print (\"-Máxima: \", np.max(hit_class_b))\n",
    "print (\"-Desvio padrão: \", np.std(hit_class_b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
